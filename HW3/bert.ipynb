{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334b25d1-5c4e-479d-8da3-0c33bbbc6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, default_data_collator, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "import evaluate\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aabba8f-1a6f-4310-8b44-412ab054dbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6718e492d15413db9771152183a869e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc9dad00d8e432ba1dd0911b277ac92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e77afec09c478d9cbf1218ffa1e44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER44 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c49c2039d5e432aa1ed5e91eb4ee828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER54 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SpokenSQuAD data completed\n"
     ]
    }
   ],
   "source": [
    "# Reformat a JSON file and save the result to a new file\n",
    "def reformat_and_save_json(json_file):\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "    except IOError as e:\n",
    "        print(f\"Error opening {json_file}: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from {json_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    examples = []\n",
    "    for elem in json_data['data']:\n",
    "        title = elem['title'].strip()\n",
    "        for paragraph in elem['paragraphs']:\n",
    "            context = paragraph['context'].strip()\n",
    "            for qa in paragraph['qas']:\n",
    "                example = {'id': qa['id'], 'title': title, 'context': context, 'question': qa['question'].strip(), 'answers': {'answer_start': [answer[\"answer_start\"] for answer in qa['answers']], 'text': [answer[\"text\"] for answer in qa['answers']]}}\n",
    "                examples.append(example)\n",
    "    \n",
    "    out_dict = {'data': examples}\n",
    "    output_json_file = os.path.join(os.path.dirname(json_file), 'formatted_' + os.path.basename(json_file))\n",
    "\n",
    "    try:\n",
    "        with open(output_json_file, 'w') as f:\n",
    "            json.dump(out_dict, f)\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to {output_json_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return output_json_file\n",
    "\n",
    "data_paths = {\n",
    "    'train': '/home/qiaoyiy/8430/models/spoken_train-v1.1.json',\n",
    "    'validation': '/home/qiaoyiy/8430/models/spoken_test-v1.1.json',\n",
    "    'test_WER44': '/home/qiaoyiy/8430/models/spoken_test-v1.1_WER44.json',\n",
    "    'test_WER54': '/home/qiaoyiy/8430/models/spoken_test-v1.1_WER54.json'\n",
    "}\n",
    "\n",
    "# Iterate over the paths of the original data files using a dictionary comprehension\n",
    "# and pass each file path to the reformat_and_save_json function.\n",
    "# This will return a new dictionary containing the paths of the processed files.\n",
    "formatted_data_paths = {key: reformat_and_save_json(path) for key, path in data_paths.items() if reformat_and_save_json(path)}\n",
    "\n",
    "# Now, formatted_data_paths contains the paths of the processed files.\n",
    "# Use these paths to load the dataset.\n",
    "spoken_squad_dataset = load_dataset('json', data_files=formatted_data_paths, field='data')\n",
    "print(\"Loading SpokenSQuAD data completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638fc1ac-7071-4020-9d8a-b137ba5381a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the BERT model and tokenizer from checkpoint 'bert-base-uncased'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'bert-base-uncased' successfully loaded for Question Answering tasks.\n",
      "Tokenizer for model 'bert-base-uncased' successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_checkpoint = \"bert-base-uncased\" \n",
    "print(f\"Loading the BERT model and tokenizer from checkpoint '{model_checkpoint}'...\") \n",
    "\n",
    "# Load the pre-trained Question Answering model from the specified checkpoint.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "print(f\"Model '{model_checkpoint}' successfully loaded for Question Answering tasks.\")\n",
    "\n",
    "# Load the tokenizer associated with the specified checkpoint.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(f\"Tokenizer for model '{model_checkpoint}' successfully loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd38530-9878-4149-aa6d-752ad5656244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing of training data with tokenization and extraction of answer positions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61db464fbd0c48b793642c63743410ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and preprocessing of validation dataset (clean data, 22.73% WER) underway...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3b84f259024ee4979312d7e715e7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test dataset with moderate noise level (44.22% WER) for evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85d8e5be649442e84807ae700248c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test dataset with high noise level (54.82% WER) for robustness assessment...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ace92444d14fa5a52c18228b1109d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define data preprocessing functions\n",
    "max_length = 384  # Maximum length of the tokenized input sequences\n",
    "stride = 64  # The stride size for splitting long documents into chunks\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop('offset_mapping')\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = answer['answer_start'][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # find start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1: \n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if answer not fully inside context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['end_positions'] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def process_validation_examples(examples):\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs['input_ids'])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offsets = inputs['offset_mapping'][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            offset if sequence_ids[k] == 1 else None for k, offset in enumerate(offsets)\n",
    "        ]\n",
    "\n",
    "    inputs['example_id'] = example_ids\n",
    "    return inputs\n",
    "\n",
    "\n",
    "print(\"Starting preprocessing of training data with tokenization and extraction of answer positions...\")\n",
    "\n",
    "train_dataset = spoken_squad_dataset['train'].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=spoken_squad_dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenization and preprocessing of validation dataset (clean data, 22.73% WER) underway...\")\n",
    "\n",
    "validation_dataset = spoken_squad_dataset['validation'].map(\n",
    "    process_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=spoken_squad_dataset['validation'].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing test dataset with moderate noise level (44.22% WER) for evaluation...\")\n",
    "\n",
    "test_WER44_dataset = spoken_squad_dataset['test_WER44'].map(\n",
    "    process_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=spoken_squad_dataset['test_WER44'].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing test dataset with high noise level (54.82% WER) for robustness assessment...\")\n",
    "\n",
    "test_WER54_dataset = spoken_squad_dataset['test_WER54'].map(\n",
    "    process_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=spoken_squad_dataset['test_WER54'].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6086b597-2846-4d91-9844-04770039436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted the training dataset to PyTorch tensor format.\n",
      "Prepared the validation dataset by removing 'example_id' and 'offset_mapping' columns and converting to PyTorch tensor format.\n",
      "Prepared the Test WER44 dataset (simulating 44% Word Error Rate) by removing unnecessary columns and converting to PyTorch tensor format.\n",
      "Prepared the Test WER54 dataset (simulating 54% Word Error Rate) by removing unnecessary columns and converting to PyTorch tensor format.\n",
      "Initializing the DataLoader for the training dataset with shuffling and a batch size of 8 to ensure varied mini-batch combinations during training.\n",
      "Initializing the DataLoader for the validation dataset with a batch size of 8 for model performance evaluation on unseen clean data.\n",
      "Initializing the DataLoader for the Test WER44 dataset with a batch size of 8 to evaluate model robustness under moderate noise conditions.\n",
      "Initializing the DataLoader for the Test WER54 dataset with a batch size of 8 to evaluate model robustness under high noise conditions.\n"
     ]
    }
   ],
   "source": [
    "# Convert the datasets to a format compatible with PyTorch models.\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "print(\"Converted the training dataset to PyTorch tensor format.\")\n",
    "\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "print(\"Prepared the validation dataset by removing 'example_id' and 'offset_mapping' columns and converting to PyTorch tensor format.\")\n",
    "\n",
    "test_WER44_set = test_WER44_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER44_set.set_format(\"torch\")\n",
    "print(\"Prepared the Test WER44 dataset (simulating 44% Word Error Rate) by removing unnecessary columns and converting to PyTorch tensor format.\")\n",
    "\n",
    "test_WER54_set = test_WER54_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER54_set.set_format(\"torch\")\n",
    "print(\"Prepared the Test WER54 dataset (simulating 54% Word Error Rate) by removing unnecessary columns and converting to PyTorch tensor format.\")\n",
    "\n",
    "print(\"Initializing the DataLoader for the training dataset with shuffling and a batch size of 8 to ensure varied mini-batch combinations during training.\")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle = True, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(\"Initializing the DataLoader for the validation dataset with a batch size of 8 for model performance evaluation on unseen clean data.\")\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
    ")\n",
    "print(\"Initializing the DataLoader for the Test WER44 dataset with a batch size of 8 to evaluate model robustness under moderate noise conditions.\")\n",
    "test_WER44_dataloader = DataLoader(\n",
    "    test_WER44_set, collate_fn=default_data_collator, batch_size=8\n",
    ")\n",
    "print(\"Initializing the DataLoader for the Test WER54 dataset with a batch size of 8 to evaluate model robustness under high noise conditions.\")\n",
    "test_WER54_dataloader = DataLoader(\n",
    "    test_WER54_set, collate_fn=default_data_collator, batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4984a36f-4387-436c-8ecb-9686129c2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics and evaluation function\n",
    "metric = evaluate.load(\"squad\")  # Load the SQuAD evaluation metric\n",
    "\n",
    "n_best = 20  # Number of top predictions to consider for each example\n",
    "max_answer_length = 30  # Maximum length of an answer that can be generated\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)  # Map each example_id to its corresponding features\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):  # Iterate through each example\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        # Loop through all features associated with an example ID\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]  # Start logit scores for this feature\n",
    "            end_logit = end_logits[feature_index]  # End logit scores for this feature\n",
    "            offsets = features[feature_index][\"offset_mapping\"]  # Token offsets for this feature\n",
    "            \n",
    "            # Get indices of the n_best start and end logits\n",
    "            start_indexes = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully within the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with invalid lengths\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    \n",
    "                    # Construct an answer candidate\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0]: offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index]\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "        \n",
    "        # Select the answer with the highest logit score\n",
    "        if answers:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "    \n",
    "    # Compare predicted answers with the actual answers\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27604671-eeb1-489b-a7b1-0d2178942632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=model, train_dataloader=train_dataloader, eval_dataloader=eval_dataloader, epochs=3):\n",
    "    print(f\"Starting model training for {epochs} epochs, each with {len(train_dataloader)} batches.\")\n",
    "    \n",
    "    training_steps = epochs * len(train_dataloader)  # Total training steps calculation\n",
    "\n",
    "    # Initialize the Accelerator for mixed precision training\n",
    "    accelerator = Accelerator(mixed_precision='fp16')\n",
    "    print(\"Accelerator initialized for mixed precision ('fp16') training.\")\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    print(\"Optimizer setup with learning rate 2e-5.\")\n",
    "\n",
    "    # Prepare model, optimizer, and dataloaders for Accelerator\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler initialization\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps,\n",
    "    )\n",
    "\n",
    "    progress_bar = tqdm(range(training_steps), desc=\"Training Progress\")  # Training progress bar setup\n",
    "\n",
    "    for epoch in range(epochs):  # Loop over epochs\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Training:\")\n",
    "        model.train()  # Set model to training mode\n",
    "        for step, batch in enumerate(train_dataloader):  # Iterate over training batches\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Model evaluation at the end of each epoch\n",
    "        print(\"\\nEvaluating model performance on the validation set...\")\n",
    "        metrics = evaluate_model(model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'], accelerator)\n",
    "        print(f\"Validation Results - Epoch {epoch+1}: {metrics}\")\n",
    "\n",
    "        # Save the model and tokenizer at the end of each epoch\n",
    "        output_dir = f\"./model_save/epoch_{epoch+1}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model_to_save = accelerator.unwrap_model(model)\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model and tokenizer saved in '{output_dir}'\")\n",
    "\n",
    "    print(\"\\nTraining completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f97593f-5d6c-4da4-92af-737a33368f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating model fine-tuning process...\n",
      "Launching training on one GPU.\n",
      "Starting model training for 3 epochs, each with 4664 batches.\n",
      "Accelerator initialized for mixed precision ('fp16') training.\n",
      "Optimizer setup with learning rate 2e-5.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f872fddc784995b08bb2b50cc64401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/13992 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 - Training:\n",
      "\n",
      "Evaluating model performance on the validation set...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9227f0c491a343eca8f031926d76750e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4561fd277d4f5cb4bc2a5b769b4564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch 1: {'exact_match': 61.50252289291721, 'f1': 72.72473191218755}\n",
      "Model and tokenizer saved in './model_save/epoch_1'\n",
      "\n",
      "Epoch 2/3 - Training:\n",
      "\n",
      "Evaluating model performance on the validation set...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2920496e0a24e769c76cf3a057d5f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b23b0adb434446be861a98bf3c74f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch 2: {'exact_match': 63.1096991216595, 'f1': 73.69561038566222}\n",
      "Model and tokenizer saved in './model_save/epoch_2'\n",
      "\n",
      "Epoch 3/3 - Training:\n",
      "\n",
      "Evaluating model performance on the validation set...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056e9e1eed91438bbeb5db2df857c606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768c9dacc774451ba9c71fd1d6772c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch 3: {'exact_match': 63.65165389646795, 'f1': 74.06165140185816}\n",
      "Model and tokenizer saved in './model_save/epoch_3'\n",
      "\n",
      "Training completed successfully.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, dataset, dataset_before_preprocessing, accelerator=None):\n",
    "    if not accelerator:\n",
    "        print(\"Initializing Accelerator for mixed precision (fp16) evaluation...\")\n",
    "        accelerator = Accelerator(mixed_precision='fp16')\n",
    "        model, dataloader = accelerator.prepare(model, dataloader)\n",
    "    \n",
    "    print(\"Setting the model to evaluation mode for performance assessment...\")\n",
    "    model.eval()\n",
    "    start_logits, end_logits = [], []\n",
    "\n",
    "    print(\"Evaluating model performance on the dataset...\")\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluation Progress\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    # Concatenate and truncate logits to align with the dataset size\n",
    "    start_logits, end_logits = np.concatenate(start_logits)[:len(dataset)], np.concatenate(end_logits)[:len(dataset)]\n",
    "\n",
    "    print(\"Computing evaluation metrics based on model predictions...\")\n",
    "    metrics = compute_metrics(start_logits, end_logits, dataset, dataset_before_preprocessing)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Initiating model fine-tuning process...\")\n",
    "notebook_launcher(train_model, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a13bc8-b50d-444b-af6a-18c252427755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Accelerator for mixed precision (fp16) evaluation...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e40722bec2440cb8376ab747650785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5e1d1681cc483a84f66f65d7adaee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Accelerator for mixed precision (fp16) evaluation...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf20b9cbff941a9a161ee18f51cb95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27f14ae8cc04c31930d60e32f71f062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Accelerator for mixed precision (fp16) evaluation...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75631495a3194a2284015bafdcc98272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19416c24cfb947d8b9d1ce471c33bdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Evaluation Summary ===\n",
      "\n",
      "+-----------------+--------------+----------+\n",
      "|     Dataset     | Exact Match  | F1 Score |\n",
      "+-----------------+--------------+----------+\n",
      "| Validation Set  |      63.65% |  74.06% |\n",
      "+-----------------+--------------+----------+\n",
      "| Test WER44 Set  |      40.83% |  55.48% |\n",
      "+-----------------+--------------+----------+\n",
      "| Test WER54 Set  |      28.41% |  42.12% |\n",
      "+-----------------+--------------+----------+\n"
     ]
    }
   ],
   "source": [
    "# Results Evaluation\n",
    "\n",
    "test_metrics = evaluate_model(model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'])\n",
    "test_wer44_metrics = evaluate_model(model, test_WER44_dataloader, test_WER44_dataset, spoken_squad_dataset['test_WER44'])\n",
    "test_wer54_metrics = evaluate_model(model, test_WER54_dataloader, test_WER54_dataset, spoken_squad_dataset['test_WER54'])\n",
    "\n",
    "print(\"\\n=== Model Evaluation Summary ===\\n\")\n",
    "\n",
    "# Headers for the results table\n",
    "headers = [\"Dataset\", \"Exact Match\", \"F1 Score\"]\n",
    "header_line = '| {:^15} | {:^12} | {:^8} |'.format(*headers)\n",
    "separator = '+' + '-' * 17 + '+' + '-' * 14 + '+' + '-' * 10 + '+'\n",
    "\n",
    "# Print table header\n",
    "print(separator)\n",
    "print(header_line)\n",
    "print(separator)\n",
    "\n",
    "# Function to format and print each row of the results table\n",
    "def print_result_row(description, metrics):\n",
    "    row = '| {:<15} | {:>10.2f}% | {:>6.2f}% |'.format(description, metrics['exact_match'], metrics['f1'])\n",
    "    print(row)\n",
    "    print(separator)\n",
    "\n",
    "# Print each result row\n",
    "print_result_row(\"Validation Set\", test_metrics)\n",
    "print_result_row(\"Test WER44 Set\", test_wer44_metrics)\n",
    "print_result_row(\"Test WER54 Set\", test_wer54_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453f957-a599-4fa4-9056-f5cbd6933bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joy",
   "language": "python",
   "name": "joy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
